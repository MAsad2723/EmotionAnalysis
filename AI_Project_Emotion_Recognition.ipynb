{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Group Members:**  \n",
        "21k-3458, Asad  \n",
        "21k-3169, Anas Imran  \n",
        "21k-4658, Zehra Fatima"
      ],
      "metadata": {
        "id": "HU9yjX0Davwk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0Sys9YS0Dsl"
      },
      "source": [
        "#EMOTIONAL ANALYSIS USING TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0ejaUjOm1Z9"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "PS9LsI5L6EOB",
        "outputId": "d03498de-f60e-4e8b-fa56-bef08695fa55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XXivuU2nS7_"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKMhk77_nWPE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "fileopen = open('drive/MyDrive/AIProjectFiles/train.txt','r') # Open the train file\n",
        "x_train_data = [] # Array for train data\n",
        "y_train_data = [] # Array for train labels\n",
        "\n",
        "#is feeling bad  just got bad news about my Mum##worry\n",
        "\n",
        "for x in fileopen: # Read train file\n",
        "    sp = x.split('##')  # Split the emotions and text\n",
        "    y_train_data.append(sp[1].strip())\n",
        "    x_train_data.append(sp[0])\n",
        "\n",
        "fileopen = open('drive/MyDrive/AIProjectFiles/test.txt','r') # Open test file\n",
        "x_test_data = [] # Array for test data\n",
        "y_test_data = [] # Array for test labels\n",
        "\n",
        "for x in fileopen: # Read test file\n",
        "    sp = x.split('##') # Split the emotion and its respective text\n",
        "    y_test_data.append(sp[1].strip())\n",
        "    x_test_data.append(sp[0])\n",
        "\n",
        "fileopen = open('drive/MyDrive/AIProjectFiles/val.txt','r') # Open validation file\n",
        "\n",
        "for x in fileopen: # Read validation file\n",
        "    sp = x.split('##') # Split the data separating the emotions and its respective text\n",
        "    y_test_data.append(sp[1].strip())\n",
        "    x_test_data.append(sp[0])\n",
        "\n",
        "# Create DataFrames for train and test data\n",
        "data_train_DF = pd.DataFrame({'Text': x_train_data, 'Emotion': y_train_data})\n",
        "# print(data_train_DF)\n",
        "data_test_DF = pd.DataFrame({'Text': x_test_data, 'Emotion': y_test_data})\n",
        "# Concatenate train and test DataFrames\n",
        "d = pd.concat([data_train_DF, data_test_DF], ignore_index=True)\n",
        "\n",
        "# Print the concatenated DataFrame\n",
        "# print(d.shape)\n",
        "# classes=pd.unique(data_train_DF['Emotion'])\n",
        "# print(len(classes))\n",
        "# classes=pd.unique(data_test_DF['Emotion'])\n",
        "# print(len(classes))\n",
        "classes=pd.unique(d['Emotion'])\n",
        "print(len(classes))\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYk-T-sbaR8U"
      },
      "outputs": [],
      "source": [
        "########## CLEANING OF DATA ##################\n",
        "def cleaning_data(d):\n",
        "    d = re.sub(r\"(#[\\d\\w\\.]+)\", '', d) #it subsitutes and irrelevant data with empty space basically cleaning of data\n",
        "    d = re.sub(r\"(@[\\d\\w\\.]+)\", '', d) #it subsitutes and irrelevant data with empty space basically cleaning of data\n",
        "    d = word_tokenize(d) #words are broken down into syllabel and stored in a variable called data\n",
        "    # print(d)\n",
        "    return d #cleaned data returned from here\n",
        "\n",
        "texts_data = [' '.join(cleaning_data(text)) for text in d.Text] #sentences are joined\n",
        "texts_train = [' '.join(cleaning_data(text)) for text in x_train_data] #cleaned data for training is stored in texts_train\n",
        "texts_test = [' '.join(cleaning_data(text)) for text in x_test_data] #clean data for testing in texts_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmGkUFiro6Wo"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer() # SPLITTING\n",
        "tokenizer.fit_on_texts(texts_data)  #This method creates the vocabulary index based on word frequency\n",
        "sequence_train = tokenizer.texts_to_sequences(texts_train) #transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index\n",
        "sequence_test = tokenizer.texts_to_sequences(texts_test) #transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index\n",
        "print(sequence_test)\n",
        "index_of_words = tokenizer.word_index #CREATES A DICT WITH INDEX NUMBER AND THAT NUMBER WAS PREVIOSULY STORED IN WORD_INDEX\n",
        "# print(index_of_words)\n",
        "vocab_size = len(index_of_words)+1 # TOTAL SIZE OF DICTINOARY VOCABULARY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['anger', 'boredom', 'empty', 'enthusiasm', 'fun', 'happiness','hate', 'love', 'neutral', 'relief', 'sadness', 'surprise','worry']\n",
        "print('{',end=\"\")\n",
        "a = 0\n",
        "for i in classes:\n",
        "  print(\"'\"+str(i)+\"':\"+str(a)+\",\", end=\"\")\n",
        "  a = a+1\n",
        "print('}')"
      ],
      "metadata": {
        "id": "wJJGufahOd85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmw_7mcYbxIo"
      },
      "outputs": [],
      "source": [
        "total_classes = len(classes)\n",
        "print(total_classes)\n",
        "embed_num_dims = 300\n",
        "seq_len = 500\n",
        "class_names = ['anger', 'boredom', 'empty', 'enthusiasm', 'fun', 'happiness','hate', 'love', 'neutral', 'relief', 'sadness', 'surprise','worry']\n",
        "X_train_pad = pad_sequences(sequence_train,maxlen=seq_len)  #CREATED A 2D ARRAY FIRST AG:sequence of integer that were previoulsy converted into words for training and second coresponds to max lenght\n",
        "X_test_pad = pad_sequences(sequence_test,maxlen=seq_len) #CREATED A 2D ARRAY FIRST AG:sequence of integer that were previoulsy converted into words for testing and second coresponds to max lenght\n",
        "encoding_data = {'anger':0,'boredom':1,'empty':2,'enthusiasm':3,'fun':4,'happiness':5,'hate':6,'love':7,'neutral':8,'relief':9,'sadness':10,'surprise':11,'worry':12}\n",
        "y_train = [encoding_data[x] for x in data_train_DF.Emotion] # All training string gets converted into integer\n",
        "y_test = [encoding_data[x] for x in data_test_DF.Emotion] #All testing strings gets converted into integer\n",
        "y_train = to_categorical(y_train) #convert the data into matrix of 0s and 1s\n",
        "y_test = to_categorical(y_test) #convert the data into matrix of 0s and 1s\n",
        "print(y_train)\n",
        "print(\"\\n\")\n",
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnq5-OTSA7R_"
      },
      "outputs": [],
      "source": [
        "! gdown --id 1SsrYFruVmyedeRuS7i8PI3SfRn6sOs0f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aM5Mv2K4b1KB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def create_matrix(filepath,index,embedding_dim): # Enter data is passed to this function to convert it into zeros and ones\n",
        "    vocab_size = len(index)+1 #size of the entire file is calculated\n",
        "    embedding_matrix = np.zeros((vocab_size,embedding_dim)) # matrix is gonna be created with vocabulary size and dimension provided\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word,*vect = line.split()\n",
        "            if word in index:\n",
        "                id = index[word]\n",
        "                embedding_matrix[id] = np.array(vect,dtype=np.float32)[:embedding_dim]\n",
        "    return embedding_matrix\n",
        "\n",
        "# fname = '/content/wiki-news-300d-1M.vec'\n",
        "fname = 'drive/MyDrive/AIProjectFiles/wiki-news-300d-1M.vec'\n",
        "embedd_matrix = create_matrix(fname,index_of_words,embed_num_dims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MApQCTzib6pU"
      },
      "outputs": [],
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  #convert each word into a fixed length vector of defined size. The resultant vector is a dense one with having real values instead of just 0's and 1's.\n",
        "  layer = Embedding(vocab_size, embed_num_dims, input_length = seq_len, weights = [embedd_matrix],trainable = False) #deep learning with keras\n",
        "  #vocab_size= size of volcabulary that we are embedding\n",
        "  #embed_num_dims= Dimension of dense embedding\n",
        "  #input_lenght=max_seq_len\n",
        "  #Turns positive integer into fixed size vectors\n",
        "  output_size = 128\n",
        "  bidirectional = True\n",
        "  model = Sequential() # PROVIDES FEATURES ON SEQUENTIAL LAYERS OF STACK\n",
        "  model.add(layer)\n",
        "  model.add(Bidirectional(GRU(units = output_size,dropout = 0.2,recurrent_dropout = 0.2)))\n",
        "  model.add(Dense(total_classes, activation = 'softmax'))\n",
        "  model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVQMMh8hjkhu"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 8\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  print(X_train_pad.shape)\n",
        "  print(y_train.shape)\n",
        "  print(X_test_pad.shape)\n",
        "  print(y_test.shape)\n",
        "  hist = model.fit(X_train_pad, y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_test_pad, y_test))\n",
        "\n",
        "# X_train = Input data\n",
        "# Y_train = Test data\n",
        "# batch_size =Number of samples per gradient\n",
        "# epochs = An epoch is an iteration over the entire x and y data provided in this case 8 times\n",
        "# validation_data = Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msg = [\"I am sad\"]\n",
        "s = tokenizer.texts_to_sequences(msg) #So it basically takes each word in the text and replaces it with its corresponding integer value\n",
        "padded = pad_sequences(s,maxlen=seq_len)#CREATED A 2D ARRAY FIRST AG:sequence of integer that were previoulsy converted into words for testing and second coresponds to max lenght\n",
        "p = model.predict(padded)# It helps you to  returns the labels of the data passed as argument based upon the learned or trained data obtained from the model.\n",
        "\n",
        "print('Message:' + str(msg))\n",
        "print('Emotion:', class_names[np.argmax(p)]) # PRINT THE EMOTIONS"
      ],
      "metadata": {
        "id": "QdONhGrW8A5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Sample data for generating emotions\n",
        "data = [\n",
        "    (\"I am happy.\", \"joy\"),\n",
        "    (\"This is sad.\", \"sadness\"),\n",
        "    (\"I feel afraid.\", \"fear\"),\n",
        "    (\"She loves him.\", \"love\"),\n",
        "    (\"What a surprise!\", \"surprise\"),\n",
        "    (\"I am angry.\", \"anger\")\n",
        "]\n",
        "\n",
        "# Generating more data items\n",
        "generated_data = []\n",
        "for _ in range(1000):\n",
        "    sentence = random.choice([\"I am\", \"This is\", \"I feel\", \"She\", \"What a\"]) + \" \" + \\\n",
        "               random.choice([\"happy.\", \"sad.\", \"afraid.\", \"loves him.\", \"surprise!\", \"angry.\"])\n",
        "    emotion = random.choice([\"joy\", \"sadness\", \"fear\", \"love\", \"surprise\", \"anger\"])\n",
        "    generated_data.append((sentence, emotion))\n",
        "\n",
        "# Splitting the generated data into train, test, and val sets\n",
        "train_data = generated_data[:700]\n",
        "test_data = generated_data[700:850]\n",
        "val_data = generated_data[850:]\n",
        "\n",
        "# Writing the data to files\n",
        "def write_to_file(file_name, data):\n",
        "    with open(file_name, 'w') as f:\n",
        "        for item in data:\n",
        "            f.write(item[0] + \";\" + item[1] + \"\\n\")\n",
        "\n",
        "write_to_file('train.txt', train_data)\n",
        "write_to_file('test.txt', test_data)\n",
        "write_to_file('val.txt', val_data)\n"
      ],
      "metadata": {
        "id": "MHcYldHgnJwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Emotions\n",
        "emotions = ['anger', 'sadness', 'fear', 'joy', 'surprise', 'love']\n",
        "\n",
        "# Function to generate random data\n",
        "def generate_data(num_samples):\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        text = ' '.join(['word' + str(random.randint(1, 100)) for _ in range(random.randint(5, 15))])\n",
        "        emotion = random.choice(emotions)\n",
        "        data.append((text, emotion))\n",
        "    return data\n",
        "\n",
        "# Generate data for train, test, and val sets\n",
        "train_data = generate_data(1000)\n",
        "test_data = generate_data(1000)\n",
        "val_data = generate_data(1000)\n",
        "\n",
        "# Function to save data to file\n",
        "def save_data(filename, data):\n",
        "    with open(filename, 'w') as file:\n",
        "        for item in data:\n",
        "            file.write(f\"{item[0]};{item[1]}\\n\")\n",
        "\n",
        "# Save data to files\n",
        "save_data('train.txt', train_data)\n",
        "save_data('test.txt', test_data)\n",
        "save_data('val.txt', val_data)\n",
        "\n",
        "print(\"Data generated and saved successfully!\")\n"
      ],
      "metadata": {
        "id": "ln0Qut2Nogyn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}